{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pytest\n",
    "import platform\n",
    "import pickle\n",
    "pd.set_option('display.max_rows',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Create aggregated data\n",
    "This section creates the aggregated and anonymised data directly from the raw data. This allows using make_table(raw=False) which creates summary tables from the raw data, before it has been anonymised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Darwin':\n",
    "    shared_drive = '/Volumes/Data/EAU/Statistics/'\n",
    "elif platform.system() == 'Windows':\n",
    "    shared_drive = 'G:/'\n",
    "# os.path.join(mydir, myfile)\n",
    "\n",
    "raw_data_dir = shared_drive + 'Economic Estimates/Employment - Helen/max-csv-data/'\n",
    "#raw_data_dir = '~/data/'\n",
    "\n",
    "years = range(2011, 2017 + 1)\n",
    "\n",
    "raw_data = {y:pd.read_csv(raw_data_dir +  'raw_' + str(y) + \"_df.csv\") for y in years}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionlookupdata = pd.read_csv('region-lookup.csv')\n",
    "regionlookdict = {row[0]: row[1] for index, row in regionlookupdata.iterrows()}\n",
    "\n",
    "sic_mappings = pd.read_csv(\"sic_mappings.csv\")\n",
    "sic_mappings = sic_mappings[sic_mappings.sic != 62.011]\n",
    "sic_mappings.sic = round(sic_mappings.sic * 100, 0)\n",
    "sic_mappings.sic = sic_mappings.sic.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = ['sex', 'ethnicity', 'dcms_ageband', 'ftpt', 'nssec']\n",
    "other_vars = ['sector', 'emptype', 'year', 'region']\n",
    "\n",
    "def expand_grid(data_dict):\n",
    "   rows = itertools.product(*data_dict.values())\n",
    "   return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n",
    "\n",
    "def clean_data(year):\n",
    "    \n",
    "    # find weighting column name for given year    \n",
    "    if year < 2016:\n",
    "        weightedcountcol = 'PWTA14'\n",
    "    if year == 2016:\n",
    "        weightedcountcol = 'PWTA16'\n",
    "    if year == 2017:\n",
    "        weightedcountcol = 'PWTA17'\n",
    "\n",
    "    df = raw_data[year]\n",
    "    \n",
    "    df['regionmain'] = df.GORWKR.map(regionlookdict)\n",
    "    df['regionsecond'] = df.GORWK2R.map(regionlookdict)\n",
    "    df['ftpt'] = df['ftpt'].astype(str)\n",
    "    df['nssec'] = df['nssec'].astype(str)\n",
    "    \n",
    "    catuniques = []\n",
    "    for caty in demographics + ['region']:\n",
    "        if caty == 'region':\n",
    "            catuniques.append(np.unique(regionlookupdata.mapno))\n",
    "        else:\n",
    "            catuniques.append(np.unique(df[caty]))\n",
    "    \n",
    "    x = pd.Series(np.unique(sic_mappings.sector))\n",
    "    y = pd.Series([\"civil_society\", \"total_uk\", \"overlap\"])\n",
    "    x = x.append(y)\n",
    "    \n",
    "    aggdict = {}\n",
    "    aggdict['sector'] = x\n",
    "    for caty in demographics + ['region']:\n",
    "        if caty == 'region':\n",
    "            aggdict[caty] = np.unique(regionlookupdata.mapno)\n",
    "        else:\n",
    "            aggdict[caty] = np.unique(df[caty])\n",
    "\n",
    "    agg = expand_grid(aggdict)\n",
    "\n",
    "    for subset in ['mainemp', 'secondemp', 'mainselfemp', 'secondselfemp']:\n",
    "        if subset == 'mainemp':\n",
    "            sicvar = \"INDC07M\"\n",
    "            emptype = \"INECAC05\"\n",
    "            emptypeflag = 1\n",
    "            regioncol = 'regionmain'\n",
    "    \n",
    "        if subset == 'secondemp':\n",
    "            sicvar = \"INDC07S\"\n",
    "            emptype = \"SECJMBR\"\n",
    "            emptypeflag = 1\n",
    "            regioncol = 'regionsecond'\n",
    "    \n",
    "        if subset == 'mainselfemp':\n",
    "            sicvar = \"INDC07M\"\n",
    "            emptype = \"INECAC05\"\n",
    "            emptypeflag = 2\n",
    "            regioncol = 'regionmain'\n",
    "    \n",
    "        if subset == 'secondselfemp':\n",
    "            sicvar = \"INDC07S\"\n",
    "            emptype = \"SECJMBR\"\n",
    "            emptypeflag = 2\n",
    "            regioncol = 'regionsecond'\n",
    "\n",
    "        # create subset for each of 4 groups\n",
    "        df['region'] = df[regioncol]\n",
    "        df['region'] = df['region'].fillna('missing region')\n",
    "        dftemp = df[[sicvar, emptype, weightedcountcol, 'cs_flag'] + demographics + ['region']].copy()\n",
    "        dftemp = dftemp.loc[dftemp[emptype] == emptypeflag]\n",
    "        # need separate sic column to allow merging - I think\n",
    "        dftemp.rename(columns={sicvar : 'sic'}, inplace=True)\n",
    "\n",
    "        # total uk includes missing sics, so take copy before removing missing sics\n",
    "        dftemp_totaluk = dftemp.copy()\n",
    "        \n",
    "        # remove rows from subset with missing sic\n",
    "        dftemp = dftemp.loc[np.isnan(dftemp.sic) == False]\n",
    "        \n",
    "        # add sector column and further subset to all sectors excluding all_dcms\n",
    "        dftemp_sectors = pd.merge(dftemp, sic_mappings.loc[:,['sic', 'sector']], how = 'inner')\n",
    "        dftemp_sectors = dftemp_sectors.loc[dftemp_sectors['sector'] != 'all_dcms']\n",
    "        \n",
    "        # subset civil society\n",
    "        dftemp_cs = dftemp.loc[dftemp['cs_flag'] == 1].copy()\n",
    "        dftemp_cs['sector'] = 'civil_society'\n",
    "        dftemp_cs = dftemp_cs[dftemp_sectors.columns.values]\n",
    "        \n",
    "        # subset all_dcms (still need to add cs and remove overlap)\n",
    "        dftemp_all_dcms = pd.merge(dftemp, sic_mappings.loc[:,['sic', 'sector']], how = 'inner')\n",
    "        dftemp_all_dcms = dftemp_all_dcms.loc[dftemp_all_dcms['sector'] == 'all_dcms']\n",
    "        \n",
    "        # subset overlap between sectors\n",
    "        dftemp_all_dcms_overlap = pd.merge(dftemp, sic_mappings.loc[:,['sic', 'sector']], how = 'inner')\n",
    "        dftemp_all_dcms_overlap = dftemp_all_dcms_overlap.loc[dftemp_all_dcms_overlap['sector'] == 'all_dcms']\n",
    "        dftemp_all_dcms_overlap = dftemp_all_dcms_overlap.loc[dftemp_all_dcms_overlap['cs_flag'] == 1]\n",
    "        dftemp_all_dcms_overlap['sector'] = 'overlap'\n",
    "        \n",
    "        # subset uk total\n",
    "        dftemp_totaluk['sector'] = 'total_uk'\n",
    "        # reorder columns\n",
    "        dftemp_totaluk = dftemp_totaluk[dftemp_sectors.columns.values]\n",
    "        \n",
    "        # append different subsets together\n",
    "        dftemp = dftemp_totaluk.append(dftemp_sectors).append(dftemp_cs).append(dftemp_all_dcms).append(dftemp_all_dcms_overlap)\n",
    "        \n",
    "        # this converts sic back to numeric\n",
    "        dftemp = dftemp.infer_objects()\n",
    "        \n",
    "        # only total_uk sector has nan sics so groupby is dropping data - setting missing values to 'missing'\n",
    "        dftemp['sic'] = dftemp['sic'].fillna(value=-1)\n",
    "\n",
    "        # create column with unique name (which is why pd.DataFrame() syntax is used) which sums the count by sector\n",
    "        aggtemp = pd.DataFrame({subset : dftemp.groupby( ['sector', 'sic'] + demographics + ['region'])[weightedcountcol].sum()}).reset_index()\n",
    "        \n",
    "        # merge final stacked subset into empty dataset containing each sector and category level combo\n",
    "        # should be able to just use aggtemp for first agg where subset=='mainemp', but gave error, need to have play around. checking that agg has all the correct sectors and cat levels should be a separate piece of code.\n",
    "        agg = pd.merge(agg, aggtemp, how='outer')\n",
    "     \n",
    "    agg['year'] = year\n",
    "    return agg\n",
    "\n",
    "def clean_data2(df):\n",
    "    \n",
    "    # fill in missing values to avoid problems with NaN\n",
    "    agg = df.fillna(0)\n",
    "    \n",
    "    # sum main and second jobs counts together\n",
    "    agg['emp'] = agg['mainemp'] + agg['secondemp']\n",
    "    agg['selfemp'] = agg['mainselfemp'] + agg['secondselfemp']\n",
    "    agg.drop(['mainemp', 'secondemp', 'mainselfemp', 'secondselfemp'], axis=1, inplace=True)\n",
    "    \n",
    "    # melt 'emp' and 'selfemp' into 'emptype'\n",
    "    agg = agg[['sector', 'sic', 'year', 'emp', 'selfemp', 'region'] + demographics]\n",
    "    melted = pd.melt(agg, id_vars=['sector', 'sic', 'year', 'region'] + demographics, var_name='emptype', value_name='count')  \n",
    "    \n",
    "    # need to aggregate before we can add civil society to all_dcms?\n",
    "    \n",
    "    # reduce down to desired aggregate\n",
    "    aggfinal = melted.drop(['sic'], axis=1)\n",
    "    aggfinal = aggfinal.groupby(['sector', 'emptype', 'year', 'region'] + demographics).sum()\n",
    "    aggfinal = aggfinal.reset_index(['sector', 'emptype', 'year', 'region'] + demographics)\n",
    "    \n",
    "    # add civil society to all_dcms and remove overlap from all_dcms\n",
    "    aggfinaloverlap = aggfinal.copy()\n",
    "    \n",
    "    alldcmsindex = aggfinaloverlap[aggfinaloverlap['sector'] == 'all_dcms'].index\n",
    "    csindex = aggfinaloverlap[aggfinaloverlap['sector'] == 'civil_society'].index\n",
    "    overlapindex = aggfinaloverlap[aggfinaloverlap['sector'] == 'overlap'].index\n",
    "    newalldcms = aggfinaloverlap.loc[alldcmsindex, ['count']].reset_index(drop=True) + aggfinaloverlap.loc[csindex, ['count']].reset_index(drop=True) - aggfinaloverlap.loc[overlapindex, ['count']].reset_index(drop=True)\n",
    "    newalldcms2 = newalldcms['count']\n",
    "    newalldcms3 = np.array(newalldcms2)\n",
    "    aggfinaloverlap.loc[alldcmsindex, ['count']] = newalldcms3\n",
    "    \n",
    "    return aggfinaloverlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run `clean_data()` on each year's data and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [clean_data(i) for i in years]\n",
    "agg = pd.concat(cleaned_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(agg.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run `clean_data2()` to convert the 'emp' columns into a single 'emptype' and 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggfinal = clean_data2(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(aggfinal.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Update individual aggregated and anonymised CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates aggregated, anonymised csv for each possible table and saves them in the directory agg_csvs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "# find all powersets - from https://docs.python.org/3/library/itertools.html#itertools-recipes\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "# create list of all combinations of \n",
    "make_table_combos = [list(x) for x in powerset(demographics + other_vars)]\n",
    "\n",
    "# define function to aggregate and anonymise data, then save to csv\n",
    "def reduce_and_save_data(combo):\n",
    "    mydf = aggfinal.groupby(combo)['count'].sum().reset_index()\n",
    "    mydf.loc[mydf['count'] < 6000, 'count'] = 0    \n",
    "    mydf.to_csv('agg_csvs/' + '_'.join(sorted(combo)) + '.csv')\n",
    "\n",
    "# run function for each combination\n",
    "for com in make_table_combos[1:]:\n",
    "    reduce_and_save_data(com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Making Tables\n",
    "This package contains aggregated, anonymised data which can be used to create summary tables. To generate/reresh this data, or produce un-anonymised summary tables see part 2 for which you will require access to the underlying raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to create summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(index, columns, sub_col=None, sub_value=None, raw=False):\n",
    "    \n",
    "    # create flag for whether data needs to be subsetted to all_dcms\n",
    "    if 'sector' not in index and 'sector' not in columns and 'sector' not in [sub_col]:\n",
    "        all_dcms = True\n",
    "    else:\n",
    "        all_dcms = False\n",
    "        \n",
    "    if raw:\n",
    "        agg_temp = aggfinal\n",
    "        \n",
    "    else:\n",
    "        # user specified subset data\n",
    "        if sub_col:\n",
    "            all_cols = index + columns + [sub_col]\n",
    "        else:\n",
    "            all_cols = index + columns\n",
    "        if all_dcms:\n",
    "            all_cols = all_cols + ['sector']\n",
    "            \n",
    "        agg_temp = pd.read_csv('agg_csvs/' + '_'.join(sorted(all_cols)) + '.csv')\n",
    "    \n",
    "    # subset data if subset arguments provided\n",
    "    if sub_col:\n",
    "            agg_temp = agg_temp.loc[agg_temp[sub_col] == sub_value]\n",
    "        \n",
    "    # for non sector breakdowns, subset data to only inlcude 'all_dcms' sector\n",
    "    if all_dcms:\n",
    "        agg_temp = agg_temp.loc[agg_temp.sector == 'all_dcms']\n",
    "        \n",
    "    # pd.crosstab() only accepts lists of series not subsetted dataframes\n",
    "    sindex = [agg_temp[col] for col in index]\n",
    "    scolumns = [agg_temp[col] for col in columns]    \n",
    "    # create table\n",
    "    tb = pd.crosstab(index=sindex, columns=scolumns, values=agg_temp['count'], aggfunc='sum')\n",
    "    \n",
    "    # reorder columns and index\n",
    "    orderings = {\n",
    "        'sector': [\"civil_society\", \"creative\", \"culture\", \"digital\", \"gambling\", \"sport\", \"telecoms\", \"all_dcms\", \"total_uk\"],\n",
    "        'sex': ['Male', 'Female'],\n",
    "        'region': ['North East', 'North West', 'Yorkshire and the Humber', 'East Midlands', 'West Midlands', 'East of England', 'London', 'South East', 'South West', 'Wales', 'Scotland', 'Northern Ireland'],\n",
    "    }\n",
    "    for i in [i for i in index if i in orderings]:\n",
    "        if isinstance(tb.index, pd.core.index.MultiIndex):\n",
    "            tb = tb.reindex(orderings[i], axis=0, level=i)\n",
    "        else:\n",
    "            tb = tb.reindex(orderings[i], axis=0)\n",
    "\n",
    "    for i in [i for i in columns if i in orderings]:\n",
    "        if isinstance(tb.columns, pd.core.index.MultiIndex):\n",
    "            tb = tb.reindex(orderings[i], axis=1, level=i)\n",
    "        else:\n",
    "            tb = tb.reindex(orderings[i], axis=1)\n",
    "    \n",
    "    \n",
    "    # round and convert to 000's\n",
    "    tb = round(tb / 1000, 0).astype(int)\n",
    "    \n",
    "    return tb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make summary tables without having access to the raw data, we need aggregated tables with columns for each aspect of the breakdown in the table. For example, if we wanted to run `make_table(['sector'], ['sex'], 'year', 2016)` then we would need an aggregate table with columns for sector, sex, and year. This package stores each possible table as a csv and reads them in as required. For example, for `make_table(['sector'], ['sex'], 'year', 2016)` we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('agg_csvs/sector_sex_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data used to create `make_table(['sector'], ['sex'], 'year', 2016)`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refresh/update this data, run Part 2 followed by Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other table examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region by employment type for 2017 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table(['emptype', 'sector'], ['region'], 'year', 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sector, employment type, and ethnicity time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_table = make_table(['ethnicity', 'sector'], ['year'])\n",
    "eth_table.to_csv('deleteme.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make summary tables without anonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table(['ethnicity'], ['region', 'emptype'], 'year', 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table(['ethnicity'], ['region', 'emptype'], 'year', 2017, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table(['emptype', 'region'], 'year', 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table(['emptype', 'sector'], ['region'], 'year', 2017, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
